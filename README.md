# LLM Roadmap 

## Running LLMs

- [ ] LLM API
- [ ] Open source LLM
- [ ] Prompt Engineering
- [ ] Structured outputs

## Building Vector Storage

- [ ] Ingesting Documents
- [ ] Splitting Documents
- [ ] Embedding Models
- [ ] Vector Databases

## RAG

- [ ] Orchestrators
- [ ] Retrievers
- [ ] Memory
- [ ] Evaluation

## Advanced RAG

- [ ] Query construction
- [ ] Agents and tools
- [ ] Post processing

## Inference Optimization

- [ ] Flash attention
- [ ] Key valve cache
- [ ] Speculative decoding

## Deploy LLMs

- [ ] Local deployment
- [ ] Demo deployment
- [ ] Server deployment
- [ ] Edge deployment

## Securing LLMs

- [ ] Prompt hacking
- [ ] Backdoors
- [ ] Defensive measures

